{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2080963,"sourceType":"datasetVersion","datasetId":1247638},{"sourceId":2088783,"sourceType":"datasetVersion","datasetId":1247557}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/alecparrott/swimming-pool-detection-training?scriptVersionId=180627958\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom PIL import Image, ImageDraw\nimport xml.etree.ElementTree as ET\nimport cv2\nfrom torch.utils.data import Dataset\nfrom pathlib import Path\nimport os\nimport random\nimport torch\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torchvision.utils import draw_bounding_boxes\nimport matplotlib.patches as patches\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Subset","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","scrolled":true,"execution":{"iopub.status.busy":"2024-05-30T14:25:55.151746Z","iopub.execute_input":"2024-05-30T14:25:55.152579Z","iopub.status.idle":"2024-05-30T14:25:59.127215Z","shell.execute_reply.started":"2024-05-30T14:25:55.152546Z","shell.execute_reply":"2024-05-30T14:25:59.126384Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"labels_dir = '/kaggle/input/swimming-pool-detection-algarves-landscape/labels/' \nimages_dir = '/kaggle/input/swimming-pool-detection-algarves-landscape/images/'\ntrain_images2_dir = '/kaggle/input/swimming-pool-detection-in-satellite-images/swimmingPool/training/images/'\ntrain_labels2_dir = '/kaggle/input/swimming-pool-detection-in-satellite-images/swimmingPool/training/labels/'\ntest_images2_dir = '/kaggle/input/swimming-pool-detection-in-satellite-images/swimmingPool/testing/images/'\ntest_labels2_dir = '/kaggle/input/swimming-pool-detection-in-satellite-images/swimmingPool/testing/labels/'\nroot_dir = '/kaggle/input/swimming-pool-detection-algarves-landscape/'","metadata":{"execution":{"iopub.status.busy":"2024-05-30T14:25:59.128763Z","iopub.execute_input":"2024-05-30T14:25:59.12924Z","iopub.status.idle":"2024-05-30T14:25:59.13407Z","shell.execute_reply.started":"2024-05-30T14:25:59.129215Z","shell.execute_reply":"2024-05-30T14:25:59.133192Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"import shutil\n\ncombined_dir = '/kaggle/working/combined_images/'\n\nif not os.path.exists(combined_dir):\n    os.mkdir(combined_dir)\n    \n    \nfor dir in [images_dir, train_images2_dir]:\n    for filename in os.listdir(dir):\n        shutil.copy(os.path.join(dir, filename), combined_dir)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T14:25:59.135224Z","iopub.execute_input":"2024-05-30T14:25:59.135486Z","iopub.status.idle":"2024-05-30T14:26:23.829574Z","shell.execute_reply.started":"2024-05-30T14:25:59.135464Z","shell.execute_reply":"2024-05-30T14:26:23.828772Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"print(len(os.listdir(combined_dir)))","metadata":{"execution":{"iopub.status.busy":"2024-05-30T14:26:23.831841Z","iopub.execute_input":"2024-05-30T14:26:23.832149Z","iopub.status.idle":"2024-05-30T14:26:23.838589Z","shell.execute_reply.started":"2024-05-30T14:26:23.832125Z","shell.execute_reply":"2024-05-30T14:26:23.83771Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"2282\n","output_type":"stream"}]},{"cell_type":"code","source":"train_transforms = A.Compose([\n    A.Resize(height=128, width=128),\n    A.RandomCrop(height=128, width=128, p=0.2),\n    A.HorizontalFlip(p=0.2),\n    A.RandomBrightnessContrast(p=0.2),\n    A.Blur(always_apply=False, p=1.0, blur_limit=(3, 7)),\n    A.HueSaturationValue(always_apply=False, p=1.0, hue_shift_limit=(-20, 20), sat_shift_limit=(-30, 30), val_shift_limit=(-20, 20)),\n    A.Rotate(limit=30, p=0.2),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2()\n], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n\ntest_transforms = A.Compose([\n    A.Resize(height=128, width=128),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),  # Match training normalization\n    ToTensorV2()\n], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))","metadata":{"execution":{"iopub.status.busy":"2024-05-30T14:26:23.839682Z","iopub.execute_input":"2024-05-30T14:26:23.839975Z","iopub.status.idle":"2024-05-30T14:26:23.85038Z","shell.execute_reply.started":"2024-05-30T14:26:23.839952Z","shell.execute_reply":"2024-05-30T14:26:23.849535Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def show_transformed_image(dataset, idx):\n    \"\"\"\n    Display a transformed image from the dataset along with its bounding boxes.\n\n    Args:\n        dataset (pool_sat_Dataset): The dataset object.\n        idx (int): The index of the image to display.\n\n    Returns:\n        None\n    \"\"\"\n    image, target = dataset[idx]\n    image = image.permute(1, 2, 0)  # Convert from CxHxW to HxWxC\n    image = image.numpy()\n    fig, ax = plt.subplots(1)\n    ax.imshow(image)\n    print(target)\n    for bbox in target['boxes']:\n        rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2]-bbox[0], bbox[3]-bbox[1], linewidth=1, edgecolor='r', facecolor='none')\n        ax.add_patch(rect)\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-30T14:26:23.851412Z","iopub.execute_input":"2024-05-30T14:26:23.851665Z","iopub.status.idle":"2024-05-30T14:26:23.860396Z","shell.execute_reply.started":"2024-05-30T14:26:23.851643Z","shell.execute_reply":"2024-05-30T14:26:23.859665Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def parse_xml(xml_file):\n    tree = ET.parse(xml_file)\n    root = tree.getroot()\n    image_data = {\n        \"filename\": root.find('filename').text,\n        \"width\": int(root.find('size/width').text),\n        \"height\": int(root.find('size/height').text),\n        \"depth\": int(root.find('size/depth').text),\n        \"xmin\": [],\n        \"ymin\": [],\n        \"xmax\": [],\n        \"ymax\": []\n    }\n\n    valid_image = True  # Assume image is valid unless a bounding box proves otherwise\n\n    for obj in root.findall('object'):\n        xmin = float(obj.find('bndbox/xmin').text)\n        ymin = float(obj.find('bndbox/ymin').text)\n        xmax = float(obj.find('bndbox/xmax').text)\n        ymax = float(obj.find('bndbox/ymax').text)\n\n        # Invalidate this image if xmin or ymin are zero\n        if xmin == 0 or ymin == 0:\n            valid_image = False\n            break  # No need to check further; one invalid box is enough\n\n        image_data[\"xmin\"].append(xmin)\n        image_data[\"ymin\"].append(ymin)\n        image_data[\"xmax\"].append(xmax)\n        image_data[\"ymax\"].append(ymax)\n\n    if not valid_image:\n        return None  # Return None to indicate this image should be skipped\n\n    return image_data","metadata":{"execution":{"iopub.status.busy":"2024-05-30T14:26:23.861376Z","iopub.execute_input":"2024-05-30T14:26:23.861627Z","iopub.status.idle":"2024-05-30T14:26:23.87378Z","shell.execute_reply.started":"2024-05-30T14:26:23.861606Z","shell.execute_reply":"2024-05-30T14:26:23.872977Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"all_images = []\n\nfor label_dir in [labels_dir, train_labels2_dir]:\n    for label_file in os.listdir(label_dir):\n        if label_file.endswith('.xml'):\n            xml_path = os.path.join(label_dir, label_file)\n            image_data = parse_xml(xml_path)\n            if image_data:  # Only add if parse_xml returns valid data\n                all_images.append(image_data)\n\ndf = pd.DataFrame(all_images)\nprint(df.head(10))\nprint(df.tail(10))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-30T14:26:23.874779Z","iopub.execute_input":"2024-05-30T14:26:23.875106Z","iopub.status.idle":"2024-05-30T14:26:40.475625Z","shell.execute_reply.started":"2024-05-30T14:26:23.875075Z","shell.execute_reply":"2024-05-30T14:26:40.474748Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"  filename  width  height  depth                              xmin  \\\n0  128.PNG    227     185      3  [17.0, 73.0, 98.0, 137.0, 163.0]   \n1  147.PNG    218     242      3                     [30.0, 125.0]   \n2  142.PNG    309     167      3                           [132.0]   \n3  127.PNG    227     186      3                            [61.0]   \n4  164.PNG    194     199      3                            [53.0]   \n5   84.PNG    115     208      3                            [48.0]   \n6  169.PNG    252     236      3                           [191.0]   \n7  166.PNG    237     148      3                            [45.0]   \n8  120.PNG    257     141      3                    [112.0, 175.0]   \n9  109.PNG    303     165      3                            [83.0]   \n\n                               ymin                                xmax  \\\n0  [21.0, 63.0, 90.0, 120.0, 144.0]  [49.0, 104.0, 129.0, 170.0, 196.0]   \n1                     [62.0, 200.0]                       [68.0, 164.0]   \n2                           [116.0]                             [168.0]   \n3                            [82.0]                              [72.0]   \n4                           [158.0]                              [86.0]   \n5                           [137.0]                              [82.0]   \n6                           [194.0]                             [228.0]   \n7                            [41.0]                              [71.0]   \n8                      [19.0, 19.0]                      [136.0, 207.0]   \n9                            [73.0]                             [117.0]   \n\n                                ymax  \n0  [55.0, 89.0, 113.0, 141.0, 168.0]  \n1                      [99.0, 223.0]  \n2                            [156.0]  \n3                            [120.0]  \n4                            [172.0]  \n5                            [175.0]  \n6                            [213.0]  \n7                             [61.0]  \n8                       [42.0, 42.0]  \n9                            [108.0]  \n           filename  width  height  depth              xmin              ymin  \\\n1308  000002865.jpg    224     224      3           [77.11]          [154.15]   \n1309  000000931.jpg    224     224      3            [16.1]          [207.28]   \n1310  000002662.jpg    224     224      3           [67.74]          [209.09]   \n1311  000002460.jpg    224     224      3   [183.36, 150.4]   [163.41, 28.08]   \n1312  000001676.jpg    224     224      3          [203.08]           [65.06]   \n1313  000000042.jpg    224     224      3  [223.73, 180.06]  [223.87, 121.86]   \n1314  000002832.jpg    224     224      3          [138.83]           [65.67]   \n1315  000001990.jpg    224     224      3    [80.6, 136.12]  [164.96, 119.35]   \n1316  000002934.jpg    224     224      3          [180.45]          [200.43]   \n1317  000002067.jpg    224     224      3    [59.3, 160.58]   [89.54, 100.75]   \n\n                  xmax              ymax  \n1308          [121.55]          [198.59]  \n1309           [59.15]           [224.0]  \n1310          [109.71]           [224.0]  \n1311   [224.0, 194.84]   [207.85, 72.53]  \n1312           [224.0]          [109.42]  \n1313    [224.0, 224.0]   [224.0, 166.31]  \n1314          [183.27]          [110.11]  \n1315  [125.05, 180.56]   [209.4, 163.79]  \n1316           [224.0]           [224.0]  \n1317  [103.74, 205.02]  [133.98, 145.19]  \n","output_type":"stream"}]},{"cell_type":"code","source":"print(df.describe())\nprint(df.isnull().sum())\n\nfor index, row in df.iterrows():\n    image_path = os.path.join(combined_dir, row['filename'])\n    if not os.path.exists(image_path):\n        print(f\"Image file {row['filename']} does not exist in {combined_dir}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-30T14:26:40.477053Z","iopub.execute_input":"2024-05-30T14:26:40.477368Z","iopub.status.idle":"2024-05-30T14:26:40.583216Z","shell.execute_reply.started":"2024-05-30T14:26:40.477342Z","shell.execute_reply":"2024-05-30T14:26:40.582381Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"             width       height   depth\ncount  1318.000000  1318.000000  1318.0\nmean    223.613050   219.369499     3.0\nstd      29.328432    24.937813     0.0\nmin      55.000000    67.000000     3.0\n25%     224.000000   224.000000     3.0\n50%     224.000000   224.000000     3.0\n75%     224.000000   224.000000     3.0\nmax     566.000000   403.000000     3.0\nfilename    0\nwidth       0\nheight      0\ndepth       0\nxmin        0\nymin        0\nxmax        0\nymax        0\ndtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"# Split the DataFrame into training and testing datasets\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T14:26:40.58729Z","iopub.execute_input":"2024-05-30T14:26:40.587538Z","iopub.status.idle":"2024-05-30T14:26:40.596834Z","shell.execute_reply.started":"2024-05-30T14:26:40.587516Z","shell.execute_reply":"2024-05-30T14:26:40.595887Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"from torchvision.transforms.functional import to_tensor\n\nclass PoolDataset(Dataset):\n    def __init__(self, dataframe, root_dir, transform=None):\n        self.dataframe = dataframe\n        self.root_dir = root_dir\n        self.transform = transform \n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        # Image loading\n        img_path = os.path.join(self.root_dir, self.dataframe.iloc[idx]['filename'])\n        image = Image.open(img_path).convert(\"RGB\")\n        image = np.array(image)  # Convert to numpy array first because using Albumentations\n\n        # Bounding boxes\n        xmin = self.dataframe.iloc[idx]['xmin']\n        ymin = self.dataframe.iloc[idx]['ymin']\n        xmax = self.dataframe.iloc[idx]['xmax']\n        ymax = self.dataframe.iloc[idx]['ymax']\n        boxes = torch.as_tensor([list(b) for b in zip(xmin, ymin, xmax, ymax)], dtype=torch.float32)\n        labels = [1] * len(boxes)\n        \n        # Labels (as long as all boxes are pools, labels are 1; background is 0 and implicit)\n        target = {'boxes': boxes, 'labels': labels}\n\n        if self.transform:\n            transformed = self.transform(image=image, bboxes=target['boxes'], labels=target['labels'])\n            image = transformed['image']\n            target['boxes'] = transformed['bboxes']\n            target['labels'] = transformed['labels']\n\n        # Convert to tensors \n        target['boxes'] = torch.as_tensor(target['boxes'], dtype=torch.float32)\n        target['labels'] = torch.as_tensor(target['labels'], dtype=torch.int64)\n\n        return image, target","metadata":{"execution":{"iopub.status.busy":"2024-05-30T14:26:40.598022Z","iopub.execute_input":"2024-05-30T14:26:40.598354Z","iopub.status.idle":"2024-05-30T14:26:40.611103Z","shell.execute_reply.started":"2024-05-30T14:26:40.598325Z","shell.execute_reply":"2024-05-30T14:26:40.610294Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch):\n    batch = [item for item in batch if item is not None and item[1]['boxes'].size(0) > 0]\n    if not batch:\n        return torch.tensor([]), []  # Return empty tensors if all items are filtered out\n    images = torch.stack([item[0] for item in batch])\n    targets = [{'boxes': item[1]['boxes'], 'labels': item[1]['labels']} for item in batch]\n    return images, targets","metadata":{"execution":{"iopub.status.busy":"2024-05-30T14:26:40.612321Z","iopub.execute_input":"2024-05-30T14:26:40.612638Z","iopub.status.idle":"2024-05-30T14:26:40.624684Z","shell.execute_reply.started":"2024-05-30T14:26:40.612608Z","shell.execute_reply":"2024-05-30T14:26:40.623962Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# Initialize dataset\ntrain_dataset = PoolDataset(train_df, combined_dir, train_transforms)\ntest_dataset = PoolDataset(test_df, combined_dir, test_transforms)\n\n# DataLoader\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\ntest_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T14:26:40.625613Z","iopub.execute_input":"2024-05-30T14:26:40.62588Z","iopub.status.idle":"2024-05-30T14:26:40.634905Z","shell.execute_reply.started":"2024-05-30T14:26:40.625858Z","shell.execute_reply":"2024-05-30T14:26:40.634097Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"import torchvision.ops as ops\n\ndef calculate_iou(boxA, boxB):\n    \"\"\"\n    Calculate the intersection over union (IoU) of two bounding boxes.\n\n    Args:\n        boxA: The first bounding box.\n        boxB: The second bounding box.\n\n    Returns:\n        The IoU of the two bounding boxes.\n    \"\"\"\n    \n    # Determine the coordinates of the intersection rectangle\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n\n    # Compute the area of intersection rectangle\n    interWidth = max(0, xB - xA)\n    interHeight = max(0, yB - yA)\n    interArea = interWidth * interHeight\n    \n    if interArea == 0:\n        return 0.0  # No overlap\n\n    # Compute the area of both bounding boxes\n    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n\n    # Compute the intersection over union\n    iou = interArea / float(boxAArea + boxBArea - interArea)\n    return iou\n\n\ndef soft_nms(boxes, scores, iou_threshold=0.5, score_threshold=0.001, sigma=0.5):\n    \"\"\"\n    Apply Soft Non-Maximum Suppression to the predictions.\n    \n    Args:\n        boxes (Tensor): The bounding boxes.\n        scores (Tensor): The confidence scores for each box.\n        iou_threshold (float): The IoU threshold for NMS. Defaults to 0.5.\n        score_threshold (float): The score threshold to discard low confidence boxes. Defaults to 0.001.\n        sigma (float): The sigma parameter for Soft-NMS. Defaults to 0.5.\n        \n    Returns:\n        Tensor: The indices of the boxes to keep.\n    \"\"\"\n    if boxes.numel() == 0:\n        return torch.tensor([], dtype=torch.int64)\n    \n    # Convert boxes to the format (x1, y1, x2, y2)\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n\n    # Calculate areas of the boxes\n    areas = (x2 - x1) * (y2 - y1)\n    \n    # Order by scores (descending)\n    order = scores.argsort(descending=True)\n\n    # List to keep track of which indices to keep\n    keep = []\n\n    while order.numel() > 0:\n        i = order[0].item()\n        keep.append(i)\n\n        if order.numel() == 1:\n            break\n        \n        # Get the IoU of the highest score box with the rest\n        xx1 = torch.max(x1[i], x1[order[1:]])\n        yy1 = torch.max(y1[i], y1[order[1:]])\n        xx2 = torch.min(x2[i], x2[order[1:]])\n        yy2 = torch.min(y2[i], y2[order[1:]])\n        \n        w = (xx2 - xx1).clamp(min=0)\n        h = (yy2 - yy1).clamp(min=0)\n        inter = w * h\n        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n\n        # Apply Soft-NMS score adjustment\n        weight = torch.exp(-(ovr ** 2) / sigma)\n        scores[order[1:]] = scores[order[1:]] * weight\n        \n        # Filter out boxes with scores below threshold\n        keep_indices = (scores[order[1:]] >= score_threshold).nonzero(as_tuple=False).squeeze()\n        \n        if keep_indices.numel() == 0:\n            break\n        \n        # Update the order tensor to process remaining boxes\n        if keep_indices.dim() == 0:  # Ensure keep_indices is a 1-D tensor\n            order = order[keep_indices + 1].unsqueeze(0)\n        else:\n            order = order[keep_indices + 1]\n        \n\n    return torch.tensor(keep, dtype=torch.int64)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T14:26:40.636213Z","iopub.execute_input":"2024-05-30T14:26:40.636444Z","iopub.status.idle":"2024-05-30T14:26:40.653647Z","shell.execute_reply.started":"2024-05-30T14:26:40.636424Z","shell.execute_reply":"2024-05-30T14:26:40.652794Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"from torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights\nfrom torch.optim.lr_scheduler import StepLR\n\nmodel = fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT,  progress=True)\nnum_classes = 2  # 1 class + background\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nnum_epochs = 15\n\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = optim.Adam(params, lr=0.0001)\nlr_scheduler = StepLR(optimizer, step_size=10, gamma=0.1)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T14:26:40.654711Z","iopub.execute_input":"2024-05-30T14:26:40.65498Z","iopub.status.idle":"2024-05-30T14:26:42.865781Z","shell.execute_reply.started":"2024-05-30T14:26:40.654959Z","shell.execute_reply":"2024-05-30T14:26:42.865017Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n100%|██████████| 160M/160M [00:01<00:00, 152MB/s]  \n","output_type":"stream"}]},{"cell_type":"code","source":"def calculate_iou(boxA, boxB):\n    \"\"\"\n    Calculate the IoU between each pair of boxes from boxA and boxB.\n    \"\"\"\n    xA = torch.max(boxA[:, None, 0], boxB[:, 0])  # Shape: [N, M]\n    yA = torch.max(boxA[:, None, 1], boxB[:, 1])  # Shape: [N, M]\n    xB = torch.min(boxA[:, None, 2], boxB[:, 2])  # Shape: [N, M]\n    yB = torch.min(boxA[:, None, 3], boxB[:, 3])  # Shape: [N, M]\n\n    interWidth = torch.clamp(xB - xA, min=0)\n    interHeight = torch.clamp(yB - yA, min=0)\n    interArea = interWidth * interHeight  # Shape: [N, M]\n\n    boxAArea = (boxA[:, 2] - boxA[:, 0]) * (boxA[:, 3] - boxA[:, 1])  # Shape: [N]\n    boxBArea = (boxB[:, 2] - boxB[:, 0]) * (boxB[:, 3] - boxB[:, 1])  # Shape: [M]\n\n    iou = interArea / (boxAArea[:, None] + boxBArea - interArea)  # Shape: [N, M]\n    return iou","metadata":{"execution":{"iopub.status.busy":"2024-05-30T14:26:42.866897Z","iopub.execute_input":"2024-05-30T14:26:42.867177Z","iopub.status.idle":"2024-05-30T14:26:42.880068Z","shell.execute_reply.started":"2024-05-30T14:26:42.867154Z","shell.execute_reply":"2024-05-30T14:26:42.879094Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\n\n# Define loss functions\nclassification_loss_fn = nn.CrossEntropyLoss()  # For classification\nregression_loss_fn = nn.SmoothL1Loss()  # For bounding box regression\n\ndef calculate_combined_loss(model_output, targets, lambda_reg=10):\n    \"\"\"\n    Calculate the combined classification and regression loss for Faster R-CNN.\n    \n    Parameters:\n    - model_output: Dict[Tensor], output from the Faster R-CNN model.\n    - targets: List[Dict], ground truth boxes and labels.\n    - lambda_reg: float, the balancing parameter for the regression loss.\n    \n    Returns:\n    - total_loss: Tensor, the combined loss for the batch.\n    \"\"\"\n    total_classification_loss = 0.0\n    total_regression_loss = 0.0\n    batch_size = len(targets)\n    \n    for i in range(batch_size):\n        # Get the model output for the i-th image\n        output_boxes = model_output['boxes'][i]\n        output_labels = model_output['labels'][i]\n        output_logits = model_output['scores'][i]\n        \n        # Get the ground truth for the i-th image\n        target_boxes = targets[i]['boxes']\n        target_labels = targets[i]['labels']\n        \n        # Calculate the classification loss\n        classification_loss = classification_loss_fn(output_logits, target_labels)\n        \n        # Calculate the regression loss only for positive samples\n        positive_indices = torch.where(target_labels > 0)[0]\n        if len(positive_indices) > 0:\n            positive_output_boxes = output_boxes[positive_indices]\n            positive_target_boxes = target_boxes[positive_indices]\n            regression_loss = regression_loss_fn(positive_output_boxes, positive_target_boxes)\n        else:\n            regression_loss = torch.tensor(0.0, device=output_boxes.device)\n        \n        # Accumulate the losses\n        total_classification_loss += classification_loss\n        total_regression_loss += regression_loss\n    \n    # Average the losses over the batch and combine them\n    avg_classification_loss = total_classification_loss / batch_size\n    avg_regression_loss = total_regression_loss / batch_size\n    total_loss = avg_classification_loss + lambda_reg * avg_regression_loss\n    \n    return total_loss\n\n# Mock ground truth targets\ntargets = [\n    {\n        'boxes': torch.tensor([[50, 30, 200, 180], [300, 350, 450, 500]], dtype=torch.float32),  # Example bounding boxes\n        'labels': torch.tensor([0, 1], dtype=torch.int64)  # Example labels (e.g., 0 for 'background', 1 for 'pool')\n    }\n]\n\n# Mock model outputs (logits for classification, assuming 2 classes)\nmodel_output = {\n    'boxes': [torch.tensor([[48, 32, 198, 178], [302, 348, 448, 498]], dtype=torch.float32)],  # Predicted boxes\n    'labels': [torch.tensor([0, 1], dtype=torch.int64)],  # Predicted labels\n    'scores': [torch.tensor([[2.0, 1.5], [1.0, 0.5]], dtype=torch.float32)]  # Confidence scores (logits for 2 classes)\n}\n\n# Test the function\nloss = calculate_combined_loss(model_output, targets)\nprint(loss)","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-05-30T14:26:42.881381Z","iopub.execute_input":"2024-05-30T14:26:42.881646Z","iopub.status.idle":"2024-05-30T14:26:42.897315Z","shell.execute_reply.started":"2024-05-30T14:26:42.881623Z","shell.execute_reply":"2024-05-30T14:26:42.896452Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"tensor(15.7241)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Training loop\ntrain_losses = []\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    running_custom_loss = 0.0\n    for images, targets in train_loader:\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        # Forward pass: get loss dictionary\n        loss_dict = model(images, targets)\n        \n        # Sum the losses from the loss dictionary\n        losses = sum(loss for loss in loss_dict.values())\n\n        # Backward pass: compute gradient and do SGD step\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n        \n        train_losses.append(losses.item())\n        running_loss += losses.item()\n    \n    # Update the learning rate\n    lr_scheduler.step()\n    \n    # Print average losses for the epoch\n    avg_loss = running_loss / len(train_loader)\n    print(f\"Epoch {epoch+1} Total Loss: {avg_loss:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-30T15:06:55.204341Z","iopub.execute_input":"2024-05-30T15:06:55.204742Z","iopub.status.idle":"2024-05-30T15:07:16.313487Z","shell.execute_reply.started":"2024-05-30T15:06:55.20471Z","shell.execute_reply":"2024-05-30T15:07:16.312307Z"},"trusted":true},"execution_count":32,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[32], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m     losses\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     21\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 23\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(\u001b[43mlosses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     24\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Update the learning rate\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plot the total loss and custom loss\nplt.figure(figsize=(10, 4))\n\n# Total Loss\nplt.subplot(1, 2, 1)\nplt.plot(train_losses)\nplt.xlabel('Iteration')\nplt.ylabel('Total Loss')\nplt.title('Total Loss Over Time')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-30T14:26:50.859557Z","iopub.status.idle":"2024-05-30T14:26:50.860017Z","shell.execute_reply.started":"2024-05-30T14:26:50.859774Z","shell.execute_reply":"2024-05-30T14:26:50.859797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'pool_detection_model_2.pth')","metadata":{"execution":{"iopub.status.busy":"2024-05-30T14:26:50.862295Z","iopub.status.idle":"2024-05-30T14:26:50.862749Z","shell.execute_reply.started":"2024-05-30T14:26:50.86252Z","shell.execute_reply":"2024-05-30T14:26:50.86254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_ground_truth_and_prediction(dataset, idx, model):\n    \"\"\"\n        Display a transformed image from the dataset along with its bounding boxes and the predicted bounding boxes.\n\n    Args:\n        dataset (pool_sat_Dataset): The dataset object.\n        idx (int): The index of the image to display.\n        model (nn.Module): The trained model.\n\n    Returns:\n        None\n    \"\"\"\n    # Get the image and target from the dataset\n    image, target = dataset[idx]\n\n    # Move the image to the device (GPU or CPU) that the model is on\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    image = image.to(device)\n\n    # Get the predicted bounding boxes from the model\n    model.eval()\n    with torch.no_grad():\n        prediction = model([image])\n\n    # Convert the image to a format that can be displayed\n    image = image.permute(1, 2, 0).cpu().numpy()\n\n    # Create a figure and axis to display the image\n    fig, ax = plt.subplots(1)\n    ax.imshow(image)\n\n    # Display the ground truth bounding boxes\n    for bbox in target['boxes']:\n        rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2]-bbox[0], bbox[3]-bbox[1], linewidth=1, edgecolor='r', facecolor='none')\n        ax.add_patch(rect)\n\n    # Display the predicted bounding boxes\n    for bbox in prediction[0]['boxes'].cpu():\n        rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2]-bbox[0], bbox[3]-bbox[1], linewidth=1, edgecolor='g', facecolor='none')\n        ax.add_patch(rect)\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-30T14:26:50.863766Z","iopub.status.idle":"2024-05-30T14:26:50.864202Z","shell.execute_reply.started":"2024-05-30T14:26:50.863965Z","shell.execute_reply":"2024-05-30T14:26:50.863982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_ground_truth_and_prediction(train_dataset, 0, model)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T14:26:50.866157Z","iopub.status.idle":"2024-05-30T14:26:50.866471Z","shell.execute_reply.started":"2024-05-30T14:26:50.866314Z","shell.execute_reply":"2024-05-30T14:26:50.866326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test loop\nmodel.eval()  # Set the model to evaluation mode\ntest_ious = []\n\nwith torch.no_grad():\n    for images, targets in test_loader:\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        # Forward pass: get predictions\n        predictions = model(images)\n        \n        for i, prediction in enumerate(predictions):\n            pred_boxes = prediction['boxes'].cpu().numpy()\n            gt_boxes = targets[i]['boxes'].cpu().numpy()\n            \n            if len(pred_boxes) == 0 or len(gt_boxes) == 0:\n                continue\n            \n            ious = []\n            for gt_box in gt_boxes:\n                for pred_box in pred_boxes:\n                    iou = calculate_iou(gt_box, pred_box)\n                    ious.append(iou)\n                    \n            if ious:\n                mean_iou = sum(ious) / len(ious)\n                test_ious.append(mean_iou)\n\n# Compute average IoU over the test set\naverage_iou = sum(test_ious) / len(test_ious) if test_ious else 0\nprint(f\"Average IoU on the test set: {average_iou:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-30T14:26:50.868512Z","iopub.status.idle":"2024-05-30T14:26:50.868858Z","shell.execute_reply.started":"2024-05-30T14:26:50.868686Z","shell.execute_reply":"2024-05-30T14:26:50.868704Z"},"trusted":true},"execution_count":null,"outputs":[]}]}