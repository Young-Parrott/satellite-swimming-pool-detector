{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2088783,"sourceType":"datasetVersion","datasetId":1247557},{"sourceId":2080963,"sourceType":"datasetVersion","datasetId":1247638}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom PIL import Image, ImageDraw\nimport xml.etree.ElementTree as ET\nimport cv2\nfrom pathlib import Path\nimport os\nimport random\nimport torch\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torchvision.utils import draw_bounding_boxes\nimport matplotlib.patches as patches\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels_dir = '/kaggle/input/swimming-pool-detection-algarves-landscape/labels/' \nimages_dir = '/kaggle/input/swimming-pool-detection-algarves-landscape/images/'\ntrain_images2_dir = '/kaggle/input/swimming-pool-detection-in-satellite-images/swimmingPool/training/images/'\ntrain_labels2_dir = '/kaggle/input/swimming-pool-detection-in-satellite-images/swimmingPool/training/labels/'\ntest_images2_dir = '/kaggle/input/swimming-pool-detection-in-satellite-images/swimmingPool/testing/images/'\ntest_labels2_dir = '/kaggle/input/swimming-pool-detection-in-satellite-images/swimmingPool/testing/labels/'\nroot_dir = '/kaggle/input/swimming-pool-detection-algarves-landscape/'","metadata":{"execution":{"iopub.status.busy":"2024-05-22T18:58:21.281912Z","iopub.execute_input":"2024-05-22T18:58:21.282560Z","iopub.status.idle":"2024-05-22T18:58:21.288165Z","shell.execute_reply.started":"2024-05-22T18:58:21.282525Z","shell.execute_reply":"2024-05-22T18:58:21.287150Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"import shutil\n\ncombined_dir = '/kaggle/working/combined_images/'\n\nif not os.path.exists(combined_dir):\n    os.mkdir(combined_dir)\n    \n    \nfor dir in [images_dir, train_images2_dir]:\n    for filename in os.listdir(dir):\n        shutil.copy(os.path.join(dir, filename), combined_dir)","metadata":{"execution":{"iopub.status.busy":"2024-05-22T18:58:21.289502Z","iopub.execute_input":"2024-05-22T18:58:21.289921Z","iopub.status.idle":"2024-05-22T18:58:23.412525Z","shell.execute_reply.started":"2024-05-22T18:58:21.289886Z","shell.execute_reply":"2024-05-22T18:58:23.411640Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"print(len(os.listdir(combined_dir)))","metadata":{"execution":{"iopub.status.busy":"2024-05-22T18:58:23.414722Z","iopub.execute_input":"2024-05-22T18:58:23.415060Z","iopub.status.idle":"2024-05-22T18:58:23.422116Z","shell.execute_reply.started":"2024-05-22T18:58:23.415032Z","shell.execute_reply":"2024-05-22T18:58:23.421110Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"2282\n","output_type":"stream"}]},{"cell_type":"code","source":"train_transforms = A.Compose([\n    A.Resize(height=128, width=128),\n    A.RandomCrop(height=128, width=128, p=0.2),\n    A.HorizontalFlip(p=0.2),\n    A.RandomBrightnessContrast(p=0.2),\n    A.Blur(always_apply=False, p=1.0, blur_limit=(3, 7)),\n    A.HueSaturationValue(always_apply=False, p=1.0, hue_shift_limit=(-20, 20), sat_shift_limit=(-30, 30), val_shift_limit=(-20, 20)),\n    A.Rotate(limit=30, p=0.2),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2()\n], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n\ntest_transforms = A.Compose([\n    A.Resize(height=128, width=128),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),  # Match training normalization\n    ToTensorV2()\n], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))","metadata":{"execution":{"iopub.status.busy":"2024-05-22T18:58:23.423581Z","iopub.execute_input":"2024-05-22T18:58:23.424345Z","iopub.status.idle":"2024-05-22T18:58:23.434159Z","shell.execute_reply.started":"2024-05-22T18:58:23.424314Z","shell.execute_reply":"2024-05-22T18:58:23.433308Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"def show_transformed_image(dataset, idx):\n    \"\"\"\n    Display a transformed image from the dataset along with its bounding boxes.\n\n    Args:\n        dataset (pool_sat_Dataset): The dataset object.\n        idx (int): The index of the image to display.\n\n    Returns:\n        None\n    \"\"\"\n    image, target = dataset[idx]\n    image = image.permute(1, 2, 0)  # Convert from CxHxW to HxWxC\n    image = image.numpy()\n    fig, ax = plt.subplots(1)\n    ax.imshow(image)\n    print(target)\n    for bbox in target['boxes']:\n        rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2]-bbox[0], bbox[3]-bbox[1], linewidth=1, edgecolor='r', facecolor='none')\n        ax.add_patch(rect)\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-22T18:58:23.435531Z","iopub.execute_input":"2024-05-22T18:58:23.435913Z","iopub.status.idle":"2024-05-22T18:58:23.444662Z","shell.execute_reply.started":"2024-05-22T18:58:23.435881Z","shell.execute_reply":"2024-05-22T18:58:23.443902Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"def parse_xml(xml_file):\n    tree = ET.parse(xml_file)\n    root = tree.getroot()\n    image_data = {\n        \"filename\": root.find('filename').text,\n        \"width\": int(root.find('size/width').text),\n        \"height\": int(root.find('size/height').text),\n        \"depth\": int(root.find('size/depth').text),\n        \"xmin\": [],\n        \"ymin\": [],\n        \"xmax\": [],\n        \"ymax\": []\n    }\n\n    valid_image = True  # Assume image is valid unless a bounding box proves otherwise\n\n    for obj in root.findall('object'):\n        xmin = float(obj.find('bndbox/xmin').text)\n        ymin = float(obj.find('bndbox/ymin').text)\n        xmax = float(obj.find('bndbox/xmax').text)\n        ymax = float(obj.find('bndbox/ymax').text)\n\n        # Invalidate this image if xmin or ymin are zero\n        if xmin == 0 or ymin == 0:\n            valid_image = False\n            break  # No need to check further; one invalid box is enough\n\n        image_data[\"xmin\"].append(xmin)\n        image_data[\"ymin\"].append(ymin)\n        image_data[\"xmax\"].append(xmax)\n        image_data[\"ymax\"].append(ymax)\n\n    if not valid_image:\n        return None  # Return None to indicate this image should be skipped\n\n    return image_data","metadata":{"execution":{"iopub.status.busy":"2024-05-22T18:58:23.445982Z","iopub.execute_input":"2024-05-22T18:58:23.446452Z","iopub.status.idle":"2024-05-22T18:58:23.459417Z","shell.execute_reply.started":"2024-05-22T18:58:23.446421Z","shell.execute_reply":"2024-05-22T18:58:23.458549Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"all_images = []\n\nfor label_dir in [labels_dir, train_labels2_dir]:\n    for label_file in os.listdir(label_dir):\n        if label_file.endswith('.xml'):\n            xml_path = os.path.join(label_dir, label_file)\n            image_data = parse_xml(xml_path)\n            if image_data:  # Only add if parse_xml returns valid data\n                all_images.append(image_data)\n\ndf = pd.DataFrame(all_images)\nprint(df.head(10))\nprint(df.tail(10))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-22T18:58:23.460397Z","iopub.execute_input":"2024-05-22T18:58:23.460650Z","iopub.status.idle":"2024-05-22T18:58:24.733881Z","shell.execute_reply.started":"2024-05-22T18:58:23.460628Z","shell.execute_reply":"2024-05-22T18:58:24.732876Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"  filename  width  height  depth                              xmin  \\\n0  128.PNG    227     185      3  [17.0, 73.0, 98.0, 137.0, 163.0]   \n1  147.PNG    218     242      3                     [30.0, 125.0]   \n2  142.PNG    309     167      3                           [132.0]   \n3  127.PNG    227     186      3                            [61.0]   \n4  164.PNG    194     199      3                            [53.0]   \n5   84.PNG    115     208      3                            [48.0]   \n6  169.PNG    252     236      3                           [191.0]   \n7  166.PNG    237     148      3                            [45.0]   \n8  120.PNG    257     141      3                    [112.0, 175.0]   \n9  109.PNG    303     165      3                            [83.0]   \n\n                               ymin                                xmax  \\\n0  [21.0, 63.0, 90.0, 120.0, 144.0]  [49.0, 104.0, 129.0, 170.0, 196.0]   \n1                     [62.0, 200.0]                       [68.0, 164.0]   \n2                           [116.0]                             [168.0]   \n3                            [82.0]                              [72.0]   \n4                           [158.0]                              [86.0]   \n5                           [137.0]                              [82.0]   \n6                           [194.0]                             [228.0]   \n7                            [41.0]                              [71.0]   \n8                      [19.0, 19.0]                      [136.0, 207.0]   \n9                            [73.0]                             [117.0]   \n\n                                ymax  \n0  [55.0, 89.0, 113.0, 141.0, 168.0]  \n1                      [99.0, 223.0]  \n2                            [156.0]  \n3                            [120.0]  \n4                            [172.0]  \n5                            [175.0]  \n6                            [213.0]  \n7                             [61.0]  \n8                       [42.0, 42.0]  \n9                            [108.0]  \n           filename  width  height  depth              xmin              ymin  \\\n1308  000002865.jpg    224     224      3           [77.11]          [154.15]   \n1309  000000931.jpg    224     224      3            [16.1]          [207.28]   \n1310  000002662.jpg    224     224      3           [67.74]          [209.09]   \n1311  000002460.jpg    224     224      3   [183.36, 150.4]   [163.41, 28.08]   \n1312  000001676.jpg    224     224      3          [203.08]           [65.06]   \n1313  000000042.jpg    224     224      3  [223.73, 180.06]  [223.87, 121.86]   \n1314  000002832.jpg    224     224      3          [138.83]           [65.67]   \n1315  000001990.jpg    224     224      3    [80.6, 136.12]  [164.96, 119.35]   \n1316  000002934.jpg    224     224      3          [180.45]          [200.43]   \n1317  000002067.jpg    224     224      3    [59.3, 160.58]   [89.54, 100.75]   \n\n                  xmax              ymax  \n1308          [121.55]          [198.59]  \n1309           [59.15]           [224.0]  \n1310          [109.71]           [224.0]  \n1311   [224.0, 194.84]   [207.85, 72.53]  \n1312           [224.0]          [109.42]  \n1313    [224.0, 224.0]   [224.0, 166.31]  \n1314          [183.27]          [110.11]  \n1315  [125.05, 180.56]   [209.4, 163.79]  \n1316           [224.0]           [224.0]  \n1317  [103.74, 205.02]  [133.98, 145.19]  \n","output_type":"stream"}]},{"cell_type":"code","source":"print(df.describe())\nprint(df.isnull().sum())\n\nfor index, row in df.iterrows():\n    image_path = os.path.join(combined_dir, row['filename'])\n    if not os.path.exists(image_path):\n        print(f\"Image file {row['filename']} does not exist in {combined_dir}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-22T18:58:24.735191Z","iopub.execute_input":"2024-05-22T18:58:24.735516Z","iopub.status.idle":"2024-05-22T18:58:24.840407Z","shell.execute_reply.started":"2024-05-22T18:58:24.735488Z","shell.execute_reply":"2024-05-22T18:58:24.839525Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"             width       height   depth\ncount  1318.000000  1318.000000  1318.0\nmean    223.613050   219.369499     3.0\nstd      29.328432    24.937813     0.0\nmin      55.000000    67.000000     3.0\n25%     224.000000   224.000000     3.0\n50%     224.000000   224.000000     3.0\n75%     224.000000   224.000000     3.0\nmax     566.000000   403.000000     3.0\nfilename    0\nwidth       0\nheight      0\ndepth       0\nxmin        0\nymin        0\nxmax        0\nymax        0\ndtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"# Split the DataFrame into training and testing datasets\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-05-22T18:58:24.843309Z","iopub.execute_input":"2024-05-22T18:58:24.843638Z","iopub.status.idle":"2024-05-22T18:58:24.849667Z","shell.execute_reply.started":"2024-05-22T18:58:24.843613Z","shell.execute_reply":"2024-05-22T18:58:24.848662Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"from torchvision.transforms.functional import to_tensor\n\nclass PoolDataset(Dataset):\n    def __init__(self, dataframe, root_dir, transform=None):\n        self.dataframe = dataframe\n        self.root_dir = root_dir\n        self.transform = transform \n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        # Image loading\n        img_path = os.path.join(self.root_dir, self.dataframe.iloc[idx]['filename'])\n        image = Image.open(img_path).convert(\"RGB\")\n        image = np.array(image)  # Convert to numpy array first because using Albumentations\n\n        # Bounding boxes\n        xmin = self.dataframe.iloc[idx]['xmin']\n        ymin = self.dataframe.iloc[idx]['ymin']\n        xmax = self.dataframe.iloc[idx]['xmax']\n        ymax = self.dataframe.iloc[idx]['ymax']\n        boxes = torch.as_tensor([list(b) for b in zip(xmin, ymin, xmax, ymax)], dtype=torch.float32)\n        labels = [1] * len(boxes)\n        \n        # Labels (as long as all boxes are pools, labels are 1; background is 0 and implicit)\n        target = {'boxes': boxes, 'labels': labels}\n\n        if self.transform:\n            transformed = self.transform(image=image, bboxes=target['boxes'], labels=target['labels'])\n            image = transformed['image']\n            target['boxes'] = transformed['bboxes']\n            target['labels'] = transformed['labels']\n\n        # Convert to tensors \n        target['boxes'] = torch.as_tensor(target['boxes'], dtype=torch.float32)\n        target['labels'] = torch.as_tensor(target['labels'], dtype=torch.int64)\n\n        return image, target","metadata":{"execution":{"iopub.status.busy":"2024-05-22T18:58:24.850858Z","iopub.execute_input":"2024-05-22T18:58:24.851152Z","iopub.status.idle":"2024-05-22T18:58:24.863136Z","shell.execute_reply.started":"2024-05-22T18:58:24.851110Z","shell.execute_reply":"2024-05-22T18:58:24.862168Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch):\n    batch = [item for item in batch if item is not None and item[1]['boxes'].size(0) > 0]\n    if not batch:\n        return torch.tensor([]), []  # Return empty tensors if all items are filtered out\n    images = torch.stack([item[0] for item in batch])\n    targets = [{'boxes': item[1]['boxes'], 'labels': item[1]['labels']} for item in batch]\n    return images, targets","metadata":{"execution":{"iopub.status.busy":"2024-05-22T18:58:24.864365Z","iopub.execute_input":"2024-05-22T18:58:24.864668Z","iopub.status.idle":"2024-05-22T18:58:24.877605Z","shell.execute_reply.started":"2024-05-22T18:58:24.864642Z","shell.execute_reply":"2024-05-22T18:58:24.876777Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# Initialize dataset\ntrain_dataset = PoolDataset(train_df, combined_dir, train_transforms)\ntest_dataset = PoolDataset(test_df, combined_dir, test_transforms)\n\n# DataLoader\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)","metadata":{"execution":{"iopub.status.busy":"2024-05-22T18:58:24.878754Z","iopub.execute_input":"2024-05-22T18:58:24.879088Z","iopub.status.idle":"2024-05-22T18:58:24.889103Z","shell.execute_reply.started":"2024-05-22T18:58:24.879056Z","shell.execute_reply":"2024-05-22T18:58:24.888214Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"import torchvision.ops as ops\n\ndef calculate_iou(boxA, boxB):\n    \"\"\"\n    Calculate the intersection over union (IoU) of two bounding boxes.\n\n    Args:\n        boxA: The first bounding box.\n        boxB: The second bounding box.\n\n    Returns:\n        The IoU of the two bounding boxes.\n    \"\"\"\n    \n    # Determine the coordinates of the intersection rectangle\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n\n    # Compute the area of intersection rectangle\n    interWidth = max(0, xB - xA)\n    interHeight = max(0, yB - yA)\n    interArea = interWidth * interHeight\n    \n    if interArea == 0:\n        return 0.0  # No overlap\n\n    # Compute the area of both bounding boxes\n    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n\n    # Compute the intersection over union\n    iou = interArea / float(boxAArea + boxBArea - interArea)\n    return iou\n\n\ndef soft_nms(boxes, scores, iou_threshold=0.5, score_threshold=0.001, sigma=0.5):\n    \"\"\"\n    Apply Soft Non-Maximum Suppression to the predictions.\n    \n    Args:\n        boxes (Tensor): The bounding boxes.\n        scores (Tensor): The confidence scores for each box.\n        iou_threshold (float): The IoU threshold for NMS. Defaults to 0.5.\n        score_threshold (float): The score threshold to discard low confidence boxes. Defaults to 0.001.\n        sigma (float): The sigma parameter for Soft-NMS. Defaults to 0.5.\n        \n    Returns:\n        Tensor: The indices of the boxes to keep.\n    \"\"\"\n    if boxes.numel() == 0:\n        return torch.tensor([], dtype=torch.int64)\n    \n    # Convert boxes to the format (x1, y1, x2, y2)\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n\n    # Calculate areas of the boxes\n    areas = (x2 - x1) * (y2 - y1)\n    \n    # Order by scores (descending)\n    order = scores.argsort(descending=True)\n\n    # List to keep track of which indices to keep\n    keep = []\n\n    while order.numel() > 0:\n        i = order[0].item()\n        keep.append(i)\n\n        if order.numel() == 1:\n            break\n        \n        # Get the IoU of the highest score box with the rest\n        xx1 = torch.max(x1[i], x1[order[1:]])\n        yy1 = torch.max(y1[i], y1[order[1:]])\n        xx2 = torch.min(x2[i], x2[order[1:]])\n        yy2 = torch.min(y2[i], y2[order[1:]])\n        \n        w = (xx2 - xx1).clamp(min=0)\n        h = (yy2 - yy1).clamp(min=0)\n        inter = w * h\n        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n\n        # Apply Soft-NMS score adjustment\n        weight = torch.exp(-(ovr ** 2) / sigma)\n        scores[order[1:]] = scores[order[1:]] * weight\n        \n        # Filter out boxes with scores below threshold\n        keep_indices = (scores[order[1:]] >= score_threshold).nonzero(as_tuple=False).squeeze()\n        \n        if keep_indices.numel() == 0:\n            break\n        \n        # Update the order tensor to process remaining boxes\n        if keep_indices.dim() == 0:  # Ensure keep_indices is a 1-D tensor\n            order = order[keep_indices + 1].unsqueeze(0)\n        else:\n            order = order[keep_indices + 1]\n        \n\n    return torch.tensor(keep, dtype=torch.int64)","metadata":{"execution":{"iopub.status.busy":"2024-05-22T18:58:24.890328Z","iopub.execute_input":"2024-05-22T18:58:24.890676Z","iopub.status.idle":"2024-05-22T18:58:24.908293Z","shell.execute_reply.started":"2024-05-22T18:58:24.890642Z","shell.execute_reply":"2024-05-22T18:58:24.907347Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"from torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights\nfrom torch.optim.lr_scheduler import StepLR\n\nmodel = fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT,  progress=True)\nnum_classes = 2  # 1 class + background\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nnum_epochs = 30\n\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = optim.Adam(params, lr=0.0001)\nlr_scheduler = StepLR(optimizer, step_size=20, gamma=0.1)","metadata":{"execution":{"iopub.status.busy":"2024-05-22T18:58:24.909288Z","iopub.execute_input":"2024-05-22T18:58:24.909526Z","iopub.status.idle":"2024-05-22T18:58:25.785878Z","shell.execute_reply.started":"2024-05-22T18:58:24.909505Z","shell.execute_reply":"2024-05-22T18:58:25.784828Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"train_losses = []\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for images, targets in train_loader:\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        # Forward pass: compute the loss\n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n\n        # Backward pass: compute gradient and do SGD step\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n        \n        train_losses.append(losses.item())\n        \n        running_loss += losses.item()\n    \n    # Update the learning rate\n    lr_scheduler.step()\n    \n    # Print average loss for the epoch\n    print(f\"Epoch {epoch+1} Loss: {running_loss / len(train_loader)}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-22T18:58:25.787137Z","iopub.execute_input":"2024-05-22T18:58:25.787443Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1 Loss: 0.2763306401444204\nEpoch 2 Loss: 0.21347897025671872\nEpoch 3 Loss: 0.19422301762934888\nEpoch 4 Loss: 0.17918242299647041\nEpoch 5 Loss: 0.1743704550194018\nEpoch 6 Loss: 0.1653418651584423\nEpoch 7 Loss: 0.15682705119252205\nEpoch 8 Loss: 0.1501675113809831\nEpoch 9 Loss: 0.13805665335420406\nEpoch 10 Loss: 0.13418074448903403\nEpoch 11 Loss: 0.13384785266085106\n","output_type":"stream"}]},{"cell_type":"code","source":"# Plot the training loss\nplt.plot(train_losses)\nplt.xlabel('Iteration')\nplt.ylabel('Training Loss')\nplt.title('Training Loss Over Time')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'pool_detection_model.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}